{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Background\n",
    "\n",
    "Suppose we have the following function or forward model,\n",
    "\n",
    "$$G(\\mathbf{x},\\mathbf{m}) = m_0 e^{m_1\\mathbf{x}} + m_2 \\mathbf{x} e^{m_3\\mathbf{x}}$$\n",
    "\n",
    "where $\\mathbf{x}$ and $\\mathbf{m}$ are vectors and the operations are element-wise, as you would expect `numpy` to do on arrays. That is to say, for $N$ length array $\\mathbf{x}$, $G$ will have $N$ values. Also assume we have data set $\\mathbf{d} = (d_0,d_1,d_2,...)$ that can explained (or 'fit') by this function. We would like to know the values of the parameters $\\mathbf{m} = (m_0,m_1,m_2,m_3)$ that best explain the data using the function $G(x,\\mathbf{m})$. Further, we would like to know the distribution of those parameters $\\mathbf{m}$.\n",
    "\n",
    "For the purpose of experimentation, let us assume that we know the true values of $\\mathbf{m_t}$ are $(1.0,-0.5,1.0,-0.75)$. Further suppose the 'data', $\\mathbf{d}$ can be generated by creating a vector $\\mathbf{x}$, and evaluating $G(\\mathbf{x},\\mathbf{m_t})$ and then adding measurement noise in the form of $N(0,\\sigma)$. Here the $\\mathbf{x}$ should be 25 equally spaced values on the interval [1,7]. The data noise should be characterized by $\\sigma = $ 0.01 where $N(0,\\sigma)$ is a Gaussian with 0 mean and standard deviation $\\sigma$. Perhaps you'll find this easier to understand in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def G(x,m):\n",
    "    \"\"\" you can do this, write the function\"\"\"\n",
    "    return x\n",
    "\n",
    "sigma = 0.01\n",
    "x = np.linspace(1,7,25)\n",
    "m_t = np.array([1,-.5,1,-.75])  # These are the true values that we will attempt to recover\n",
    "\n",
    "d = G(x,m_t) + np.random.randn(x.size) * sigma  # The 'data' which we generate with a noise signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you should now know, the sampling will require a so-called likelihood function, or the probability of encountering the data, given some set of $\\mathbf{m}$. If we assume errors are Gaussian, or normally distributed, as we did for the noise on the observations, then the likelihood will be\n",
    "\n",
    "$$q(\\mathbf{d} | \\mathbf{m}) = \\prod_{i=1}^n e^{-\\frac{1}{2}(d_i - G(x_i,\\mathbf{m}))^2 / \\sigma^2}$$\n",
    "\n",
    "This is great, but we are taking a product of many small numbers here and the potential for underflow is real. So, we take the natural $\\log$ of this:\n",
    "\n",
    "$$\\log(q(\\mathbf{d}|\\mathbf{m})) = -\\frac{1}{2} \\sum_{i=1}^n (d_i - G(x_i,\\mathbf{m})^2/\\sigma^2$$\n",
    "\n",
    "The acceptance rates are in general\n",
    "\n",
    "$$\\alpha(\\mathbf{m_p},\\mathbf{m}) = \\min\\left(1,\\frac{q(\\mathbf{d} | \\mathbf{m_p})}{q(\\mathbf{d} | \\mathbf{m})}\\right)$$\n",
    "\n",
    "Taking the $\\log$ of the acceptance rates gives\n",
    "\n",
    "$$ \\log(\\alpha) = \\min(0,\\log(q(\\mathbf{d}|\\mathbf{m_p})) -\\log(q(\\mathbf{d}|\\mathbf{m})))$$\n",
    "\n",
    "where $\\mathbf{m}_p$ is the proposed $\\mathbf{m}$. Don't forget that because the acceptance rate is the $\\log$ of liklihood calculations, then we must take the $\\log$ of the uniform random number from the interval [0,1] as well.\n",
    "\n",
    "### Problem ###\n",
    "\n",
    "Determine distributions for the parameters $\\mathbf{m}$ by using MCMC to sample the posterior distributions. Use uniformly distributed random numbers for $\\mathbf{m}_o$, the intial values of the parameters: $m_0 \\in [0,2]$, $m_1 \\in [-1,0]$, $m_2 \\in [0,2]$, $m_3 \\in [-1,0]$. Do a \"burn-in\" of 10,000 samples, followed by a chain of 400,000 samples. In each MCMC step, generate a proposal by adding normally distributed random numbers with a mean of zero and standard deviation of 0.005 to $\\mathbf{m}$.\n",
    "\n",
    "Once the simulation is complete, resample the MCMC chain by taking only every 1000th set of $\\mathbf{m}$ from the full chain. Plot histograms for each variable, and compute means and standard deviations on the resulting distributions. How close are you to 'truth'?\n",
    "\n",
    "Consider the following question: would it be better to double the size of the data set, or halve the errors in the data?\n",
    "\n",
    "#### Hints ####\n",
    "Syntacitically, the `scipy.stats.norm` is often cleaner than `numpy.random.randn` for generating random numbers.\n",
    "\n",
    "Check and recheck your formula for the likelihood. It's critical but easy to make mistakes on parentheses.\n",
    "\n",
    "It takes a long time to run 400,000 samples. For testing just do 100,000. You'll get the sense of things from that.\n",
    "\n",
    "The acceptance rate is around 40%. Compute it and check against this value to see if you're doing things right.\n",
    "\n",
    "Truthfully, I had a hard time getting this to work right. The forward model is very sensitive to small changes. Don't kill yourself on this assignment. Something that looks close will grade well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
